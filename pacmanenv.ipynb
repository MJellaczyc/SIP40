{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Środowisko gry Pacman\n",
    "import gym \n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "from run import GameController\n",
    "from constants import *\n",
    "from pacman import Pacman\n",
    "from ghost import Ghosts\n",
    "from nodes import NodeGroup\n",
    "from pellets import PelletGroup\n",
    "from fruits import Fruits\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "\n",
    "class PacmanEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "class PacmanEnv(gym.Env):\n",
    "    def __init__(self, render_mode=False):\n",
    "        super(PacmanEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.time_limit = 300\n",
    "        self.elapsed_time = 0 \n",
    "\n",
    "        if not render_mode:\n",
    "            os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "        else:\n",
    "            os.environ.pop(\"SDL_VIDEODRIVER\", None)\n",
    "\n",
    "        pygame.quit()\n",
    "        pygame.init()\n",
    "\n",
    "        \n",
    "        self.game = GameController(render_mode=render_mode)\n",
    "\n",
    "        self.action_space = spaces.Discrete(5, start=-2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255, shape=(SCREENHEIGHT, SCREENWIDTH, 3), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.game.startGame(3)  # Rozpoczęcie gry\n",
    "        state = self.get_observation()  # Pobranie stanu początkowego\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        action = int(action)  # Konwersja akcji na int\n",
    "        action = np.clip(action, 0, 4)\n",
    "    \n",
    "        # Mappowanie akcji z {0, 1, 2, 3, 4} na {-2, -1, 0, 1, 2}\n",
    "        action = action - 2\n",
    "        if self.game.pacman.validDirection(action):\n",
    "            self.game.pacman.direction = action \n",
    "\n",
    "        pelletBefore = self.game.pellets.numEaten \n",
    "        lifesBefore = self.game.pacman.life_amount   \n",
    "\n",
    "        self.game.update()\n",
    "        self.elapsed_time += self.game.clock.get_time() / 1000\n",
    "\n",
    "        if self.game.pacman.target is not None and self.game.pacman.overshotTarget():\n",
    "            self.game.pacman.node = self.game.pacman.target\n",
    "            self.game.pacman.setPosition()\n",
    "\n",
    "        self.game.update()\n",
    "\n",
    "        state = self.get_observation()\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        pellet = self.game.pellets.numEaten - pelletBefore\n",
    "        if pellet == 1:\n",
    "            reward += 20\n",
    "\n",
    "        #liczenie za owocki nie działa drodzy panstwo\n",
    "        fruit = None\n",
    "        if self.game.fruits is not None:\n",
    "            fruit = self.game.pacman.eatFruits(self.game.fruits)\n",
    "            if fruit:\n",
    "                reward += 20\n",
    "\n",
    "        \n",
    "        lifes = self.game.pacman.life_amount - lifesBefore\n",
    "        if lifes == -1:\n",
    "            reward -= 50\n",
    "\n",
    "        \n",
    "        if pellet == 0 and fruit is None:\n",
    "            reward -= 2\n",
    "\n",
    "        done = self.check_game_over()\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        if self.render_mode and mode == \"human\":\n",
    "            self.game.render()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return pygame.surfarray.array3d(self.game.screen)\n",
    "\n",
    "    def _init_pygame(self):\n",
    "        if not pygame.get_init():\n",
    "            pygame.init()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n",
    "    def check_game_over(self):\n",
    "        game_over = (\n",
    "          self.game.pacman.life_amount == 0 or\n",
    "          self.game.pellets.isEmpty() or\n",
    "          self.elapsed_time >= self.time_limit\n",
    "        )\n",
    "        if game_over:\n",
    "          self.elapsed_time = 0\n",
    "        return game_over\n",
    "    \n",
    "    def change_resolution(self, width, height):\n",
    "        global SCREENWIDTH, SCREENHEIGHT\n",
    "\n",
    "        constants_path = os.path.join(os.path.dirname(__file__), \"constants.py\")\n",
    "        with open(constants_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(constants_path, \"w\") as file:\n",
    "            for line in lines:\n",
    "                if line.startswith(\"SCREENWIDTH\"):\n",
    "                    file.write(f\"SCREENWIDTH = {width}\\n\")\n",
    "                elif line.startswith(\"SCREENHEIGHT\"):\n",
    "                    file.write(f\"SCREENHEIGHT = {height}\\n\")\n",
    "                else:\n",
    "                    file.write(line)\n",
    "\n",
    "        SCREENWIDTH, SCREENHEIGHT = width, height\n",
    "        \n",
    "        self.game.screen = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "        self.game.width, self.game.height = SCREENWIDTH, SCREENHEIGHT\n",
    "\n",
    "    def get_observation(self):\n",
    "        observation = pygame.surfarray.array3d(self.game.screen)\n",
    "        return np.transpose(observation, (1, 0, 2))  # Zamienia wymiary: (800, 600, 3) na (600, 800, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BADANIE 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym  # Gymnasium zamiast gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import DQN, A2C, PPO  # Algorytmy RL\n",
    "\n",
    "def test_model(model, env, episodes=15):\n",
    "    rewards = []\n",
    "    durations = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(state)\n",
    "            result = env.step(action)\n",
    "            \n",
    "            if len(result) == 5:\n",
    "                state, reward, terminated, truncated, _ = result\n",
    "            else: \n",
    "                state, reward, done, _ = result\n",
    "                terminated = done\n",
    "                truncated = False\n",
    "\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        durations.append(steps)\n",
    "\n",
    "    return rewards, durations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def make_env():\n",
    "        return gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "    env = DummyVecEnv([make_env])\n",
    "\n",
    "    dqn_model = DQN.load(\"ms_pacman_dqn_gym\", env=env)\n",
    "    a2c_model = A2C.load(\"ms_pacman_a2c_gym\", env=env)\n",
    "    ppo_model = PPO.load(\"ms_pacman_ppo_gym\", env=env)\n",
    "\n",
    "    test_episodes = 15\n",
    "\n",
    "    print(\"Testing DQN model...\")\n",
    "    dqn_rewards, dqn_durations = test_model(dqn_model, env, episodes=test_episodes)\n",
    "\n",
    "    print(\"Testing A2C model...\")\n",
    "    a2c_rewards, a2c_durations = test_model(a2c_model, env, episodes=test_episodes)\n",
    "\n",
    "    print(\"Testing PPO model...\")\n",
    "    ppo_rewards, ppo_durations = test_model(ppo_model, env, episodes=test_episodes)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"DQN - Średnia liczba punktów: {np.mean(dqn_rewards):.2f}, Średni czas trwania: {np.mean(dqn_durations):.2f}\")\n",
    "    print(f\"A2C - Średnia liczba punktów: {np.mean(a2c_rewards):.2f}, Średni czas trwania: {np.mean(a2c_durations):.2f}\")\n",
    "    print(f\"PPO - Średnia liczba punktów: {np.mean(ppo_rewards):.2f}, Średni czas trwania: {np.mean(ppo_durations):.2f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(dqn_rewards, label=\"DQN\", color='blue')\n",
    "    plt.plot(a2c_rewards, label=\"A2C\", color='orange')\n",
    "    plt.plot(ppo_rewards, label=\"PPO\", color='green')\n",
    "    plt.title(\"Liczba punktów w kolejnych epizodach\")\n",
    "    plt.xlabel(\"Epizod\")\n",
    "    plt.ylabel(\"Punkty\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(dqn_durations, label=\"DQN\", color='blue')\n",
    "    plt.plot(a2c_durations, label=\"A2C\", color='orange')\n",
    "    plt.plot(ppo_durations, label=\"PPO\", color='green')\n",
    "    plt.title(\"Czas trwania epizodów\")\n",
    "    plt.xlabel(\"Epizod\")\n",
    "    plt.ylabel(\"Liczba kroków\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BADANIE 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "# Funkcja testująca model\n",
    "def evaluate_model(model, env, episodes=15):\n",
    "    rewards = []\n",
    "    times = []\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(f\"Epizod {i + 1}\")\n",
    "        observation = env.reset()\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        start_time = time.time()\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(observation, deterministic=True)\n",
    "            result = env.step(action)\n",
    "\n",
    "            if len(result) == 5:\n",
    "                observation, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                observation, reward, done, _ = result\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "        times.append(time.time() - start_time)\n",
    "        rewards.append(episode_reward)\n",
    "\n",
    "    return np.array(rewards), np.array(times)\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecTransposeImage(env)\n",
    "\n",
    "models = {\n",
    "    \"DQN\": DQN.load(\"ms_pacman_dqn_gym\", env=env),\n",
    "    \"PPO\": PPO.load(\"ms_pacman_ppo_gym\", env=env),\n",
    "    \"A2C\": A2C.load(\"ms_pacman_a2c_gym\", env=env),\n",
    "}\n",
    "\n",
    "test_results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTestowanie modelu: {model_name}\")\n",
    "    rewards, times = evaluate_model(model, env)\n",
    "    test_results[model_name] = {\n",
    "        \"rewards\": rewards,\n",
    "        \"times\": times,\n",
    "        \"mean_reward\": np.mean(rewards),\n",
    "        \"max_reward\": np.max(rewards),\n",
    "        \"min_reward\": np.min(rewards),\n",
    "        \"std_reward\": np.std(rewards),\n",
    "        \"mean_time\": np.mean(times),\n",
    "    }\n",
    "\n",
    "\n",
    "for model_name, result in test_results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Średnia nagroda: {result['mean_reward']:.2f}\")\n",
    "    print(f\"Maksymalna nagroda: {result['max_reward']:.2f}\")\n",
    "    print(f\"Minimalna nagroda: {result['min_reward']:.2f}\")\n",
    "    print(f\"Odchylenie standardowe nagród: {result['std_reward']:.2f}\")\n",
    "    print(f\"Średni czas decyzji: {result['mean_time']:.4f} sekundy\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model_name, result in test_results.items():\n",
    "    plt.plot(result[\"rewards\"], label=f'{model_name} - nagrody')\n",
    "plt.xlabel(\"Epizod\")\n",
    "plt.ylabel(\"Nagroda\")\n",
    "plt.title(\"Krzywa nagród modeli Pacmana\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(test_results.keys())\n",
    "mean_rewards = [test_results[model][\"mean_reward\"] for model in model_names]\n",
    "plt.bar(model_names, mean_rewards, color=['red', 'green', 'blue'])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Średnia nagroda\")\n",
    "plt.title(\"Porównanie średnich nagród modeli Pacmana\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "mean_times = [test_results[model][\"mean_time\"] for model in model_names]\n",
    "plt.bar(model_names, mean_times, color=['cyan', 'magenta', 'yellow'])\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Średni czas decyzji (s)\")\n",
    "plt.title(\"Porównanie średnich czasów decyzji modeli Pacmana\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for model_name, result in test_results.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(result[\"rewards\"], bins=20, alpha=0.7, color='orange')\n",
    "    plt.xlabel(\"Nagroda\")\n",
    "    plt.ylabel(\"Liczba epizodów\")\n",
    "    plt.title(f\"Histogram nagród dla modelu {model_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU) is not available. Make sure your NVIDIA drivers and CUDA toolkit are properly installed.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.88GB > 1.96GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | 564      |\n",
      "|    exploration_rate | 0.845    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 1826     |\n",
      "|    total_timesteps  | 817      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.09     |\n",
      "|    n_updates        | 179      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 174      |\n",
      "|    ep_rew_mean      | 637      |\n",
      "|    exploration_rate | 0.735    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 3024     |\n",
      "|    total_timesteps  | 1395     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.41     |\n",
      "|    n_updates        | 323      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 171      |\n",
      "|    ep_rew_mean      | 608      |\n",
      "|    exploration_rate | 0.61     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 4397     |\n",
      "|    total_timesteps  | 2053     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.6      |\n",
      "|    n_updates        | 488      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 174      |\n",
      "|    ep_rew_mean      | 633      |\n",
      "|    exploration_rate | 0.47     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 0        |\n",
      "|    time_elapsed     | 5966     |\n",
      "|    total_timesteps  | 2792     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.86     |\n",
      "|    n_updates        | 672      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Trening modelu\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpacman_dqn_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Zapisanie wytrenowanego modelu\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()  \u001b[38;5;66;03m# Zamknięcie środowiska treningowego\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:217\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Optimize the policy\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 217\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Clip gradient norm\u001b[39;00m\n\u001b[0;32m    219\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\48661\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Nasze środowisko, uczenie modelu DQN\n",
    "if __name__ == \"__main__\":\n",
    "    from stable_baselines3 import DQN\n",
    "\n",
    "    env = PacmanEnv(render_mode=False)\n",
    "\n",
    "    model = DQN(\n",
    "        \"CnnPolicy\", \n",
    "        env, \n",
    "        verbose=1, \n",
    "        buffer_size=1000, \n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    train = True\n",
    "\n",
    "    if train:\n",
    "        print(\"Training the model...\")\n",
    "        model.learn(total_timesteps=50000)\n",
    "        model.save(\"pacman_dqn_model\")\n",
    "        env.close()\n",
    "\n",
    "        print(\"Switching to testing mode...\")\n",
    "        env = PacmanEnv(render_mode=True)\n",
    "        state = env.reset()\n",
    "        rewardMain = 0\n",
    "        print(\"Trained model:\")\n",
    "\n",
    "    else:\n",
    "        print(\"Switching to testing mode...\")\n",
    "        env = PacmanEnv(render_mode=True)\n",
    "        model = DQN.load(\"pacman_dqn_model\", env=env, device=\"cuda\")\n",
    "        state = env.reset()\n",
    "        rewardMain = 0\n",
    "        print(\"Trained model:\")\n",
    "\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(state)\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        rewardMain += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Game Over\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total reward during testing: {rewardMain}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Nasze środowisko, uczenie modelu PPO\n",
    "if __name__ == \"__main__\":\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "    env = PacmanEnv(render_mode=False)\n",
    "    \n",
    "    model = PPO(\"CnnPolicy\", env, verbose=1, n_steps=256, batch_size=64, ent_coef=0.01)\n",
    "\n",
    "    print(\"Training the PPO model...\")\n",
    "    model.learn(total_timesteps=100)\n",
    "    model.save(\"pacman_ppo_model\")\n",
    "    env.close()\n",
    "\n",
    "    print(\"Switching to testing mode...\")\n",
    "    env = PacmanEnv(render_mode=True)\n",
    "\n",
    "    model = PPO.load(\"pacman_ppo_model\", env=env)\n",
    "\n",
    "    state = env.reset()\n",
    "    rewardMain = 0\n",
    "    print(\"Trained PPO model:\")\n",
    "\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(state)\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        rewardMain += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Game Over\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total reward during testing: {rewardMain}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Nasze środowisko, uczenie modelu A2C\n",
    "if __name__ == \"__main__\":\n",
    "    from stable_baselines3 import A2C\n",
    "    from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "    env = PacmanEnv(render_mode=False)\n",
    "    \n",
    "    model = A2C(\"CnnPolicy\", env, verbose=1, n_steps=5, ent_coef=0.01, learning_rate=0.0007, gamma=0.99)\n",
    "\n",
    "    print(\"Training the A2C model...\")\n",
    "    model.learn(total_timesteps=100)\n",
    "    model.save(\"pacman_a2c_model\")\n",
    "    env.close()\n",
    "\n",
    "    print(\"Switching to testing mode...\")\n",
    "    env = PacmanEnv(render_mode=True)\n",
    "    \n",
    "    model = A2C.load(\"pacman_a2c_model\", env=env)\n",
    "\n",
    "    state = env.reset()\n",
    "    rewardMain = 0\n",
    "    print(\"Trained A2C model:\")\n",
    "\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(state)\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        rewardMain += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Game Over\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total reward during testing: {rewardMain}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Środowisko gry Pacman z OpenAI Gym\n",
    "#Aby wytrenować pozostałe modele, należy zmienić nazwe na odpowiedni model DQN/A2C/PPO i wziąc parametry takie same jak przy trenowaniu modelu przy zaimplementowanym naszym środowisku\n",
    "import gym\n",
    "import ale_py\n",
    "import gym.envs\n",
    "import gym.envs.registration\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    return gym.make(\"ALE/MsPacman-v5\")\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "\n",
    "model = DQN('CnnPolicy', env, verbose=1, learning_rate=1e-4, buffer_size=50000)\n",
    "model.learn(total_timesteps=50000)\n",
    "model.save(\"ms_pacman_dqn_gym\")\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "obs = env.reset()\n",
    "done = False\n",
    "rewardMain = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    rewardMain += reward\n",
    "    env.render()\n",
    "\n",
    "print(f\"Total reward during testing: {rewardMain}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
